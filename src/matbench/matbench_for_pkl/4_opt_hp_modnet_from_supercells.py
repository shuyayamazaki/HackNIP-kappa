#!/usr/bin/env python3
"""
Hyper-parameter optimisation for MODNet models trained on supercell feature bundles.

This script mirrors the behaviour of 6_opt_hp_modnet.py but is tailored to the
split-aware data generated by build_supercells_from_pkl.py and
featurize_construct_from_supercells.py. It consumes the train/test feature
pickles (e.g. <slug>_train_XPS_orb2.pkl) and performs Optuna-based tuning for
each requested GNN layer on the designated training split, reporting the best
configuration and evaluating it on the held-out test split.
"""

import argparse
import csv
import json
import os
import pickle
import random
import re
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple

import numpy as np
import optuna
import pandas as pd
import tensorflow as tf
import torch
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import KFold

from modnet.models import MODNetModel
from modnet.preprocessing import MODData
from run_benchmark import parse_task_list


DATA_ROOT = Path(
    os.environ.get("BENCH_DATA_DIR", Path(__file__).resolve().parent / "benchmark_data")
).resolve()
MLIP = os.environ.get("BENCH_MLIP", "orb2")
MODEL = os.environ.get("BENCH_MODEL", "modnet")
TASKS_ENV = os.environ.get("BENCH_TASKS", "")

STRUCTURES_DIR = DATA_ROOT / "structures"
META_DIR = DATA_ROOT / "metadata"
FEAT_DIR = DATA_ROOT / f"feat_{MLIP}"
NPY_DIR = FEAT_DIR / "npy"
RESULTS_DIR = FEAT_DIR / f"results_{MODEL}"
HP_DIR = RESULTS_DIR / "hp"
PARITY_DIR = RESULTS_DIR / "parity"
TRIAL_MODELS_DIR = RESULTS_DIR / "trial_models"

for directory in [
    STRUCTURES_DIR,
    META_DIR,
    FEAT_DIR,
    NPY_DIR,
    RESULTS_DIR,
    HP_DIR,
    PARITY_DIR,
    TRIAL_MODELS_DIR,
]:
    directory.mkdir(parents=True, exist_ok=True)

DEFAULT_KEY = "XPS"
DEFAULT_TARGET_NAME = "g"
DEFAULT_LAYERS = range(1, 16)
DEFAULT_OPTUNA_TRIALS = 50
MATBENCH_SEED = 18012019
DEFAULT_TRAIN_SPLIT = "train"
DEFAULT_TEST_SPLIT = "test"

MODEL_STEM_PATTERN = re.compile(
    r"(?P<slug>.+?)_(?P<train>[A-Za-z0-9]+)2(?P<test>[A-Za-z0-9]+)_(?P<key>[A-Za-z0-9]+)_(?P<mlip>[A-Za-z0-9]+)_l(?P<layer>\d+)$"
)
SIMPLE_MODEL_STEM_PATTERN = re.compile(
    r"(?P<slug>.+?)_(?P<key>[A-Za-z0-9]+)_(?P<mlip>[A-Za-z0-9]+)_l(?P<layer>\d+)$"
)


def configure_devices(cuda_visible: Optional[str]) -> None:
    """Configure CUDA visibility and ensure TF GPU memory growth is enabled."""
    if cuda_visible is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = cuda_visible
    gpus = tf.config.list_physical_devices("GPU")
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] PyTorch device: {device}")
    print(f"[INFO] TensorFlow GPUs: {tf.config.list_logical_devices('GPU')}")


def set_seed(seed: int) -> None:
    """Seed Python, NumPy, Torch, and TensorFlow RNGs."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    tf.random.set_seed(seed)


def parse_slug_list(spec: Optional[str]) -> List[str]:
    """Parse a comma-separated list of dataset slugs."""
    if spec is None:
        return []
    slugs: List[str] = []
    seen = set()
    for token in spec.split(","):
        slug = token.strip()
        if not slug or slug in seen:
            continue
        slugs.append(slug)
        seen.add(slug)
    return slugs


def load_feature_bundle(slug: str, key: str, split: Optional[str]) -> Tuple[Dict, Path]:
    """Load feature bundle for the dataset/ split."""
    filename = f"{slug}_{key}_{MLIP}.pkl" if split is None else f"{slug}_{split}_{key}_{MLIP}.pkl"
    path = FEAT_DIR / filename
    if not path.exists():
        raise FileNotFoundError(f"Feature bundle not found: {path}")
    with open(path, "rb") as fp:
        bundle = pickle.load(fp)
    return bundle, path


def detect_available_layers(bundle: Dict, key: str) -> List[int]:
    """Detect available layers (supports both XPS_lN and XPS_N)."""
    prefix_with_l = f"{key}_l"
    prefix_without = f"{key}_"
    layers: List[int] = []
    for feat_key in bundle.keys():
        suffix = None
        if feat_key.startswith(prefix_with_l):
            suffix = feat_key[len(prefix_with_l) :]
        elif feat_key.startswith(prefix_without):
            suffix = feat_key[len(prefix_without) :]
        if suffix is None or not suffix.isdigit():
            continue
        idx = int(suffix)
        if idx not in layers:
            layers.append(idx)
    return sorted(layers)


def get_layer_features(bundle: Dict, key: str, layer: int) -> np.ndarray:
    """Return feature matrix for specified layer."""
    candidates = [f"{key}_l{layer}", f"{key}_{layer}"]
    for candidate in candidates:
        if candidate in bundle:
            return np.asarray(bundle[candidate], dtype=np.float32)
    raise KeyError(f"Layer {layer} not found (tried {candidates}).")


def flatten_predictions(pred: np.ndarray) -> np.ndarray:
    """Ensure MODNet predictions are 1-D arrays."""
    arr = np.asarray(pred)
    if arr.ndim == 2:
        arr = arr[:, 0]
    return arr.reshape(-1)


def make_moddata(matrix: np.ndarray, targets: np.ndarray, target_name: str) -> MODData:
    """Wrap numpy arrays into MODData with feature labels."""
    df = pd.DataFrame(matrix)
    series = pd.Series(targets)
    moddata = MODData(df_featurized=df, targets=series, target_names=[target_name])
    moddata.optimal_features = list(df.columns)
    return moddata


def parse_model_stem(stem: str) -> Dict[str, object]:
    """Parse a model filename stem into slug, splits, key, mlip, and layer."""
    match = MODEL_STEM_PATTERN.match(stem)
    if match:
        data = match.groupdict()
        train_split = data["train"]
        test_split = data["test"]
    else:
        simple_match = SIMPLE_MODEL_STEM_PATTERN.match(stem)
        if not simple_match:
            raise ValueError(
                "Cannot infer slug/key/layer from model filename "
                f"'{stem}'. Expected pattern '<slug>_<train>2<test>_<KEY>_<MLIP>_l<number>' "
                "or '<slug>_<KEY>_<MLIP>_l<number>'."
            )
        data = simple_match.groupdict()
        train_split = DEFAULT_TRAIN_SPLIT
        test_split = DEFAULT_TEST_SPLIT

    return {
        "slug": data["slug"],
        "train_split": train_split,
        "test_split": test_split,
        "key": data["key"],
        "mlip": data["mlip"],
        "layer": int(data["layer"]),
    }


def extract_context_from_model_path(model_path: Path) -> Dict[str, object]:
    """Extract dataset context information from a MODNet model path."""
    context = parse_model_stem(model_path.stem)
    # Normalise casing
    context["key"] = str(context["key"]).upper()
    context["mlip"] = str(context["mlip"])
    return context


def locate_training_run_root(path: Path) -> Optional[Path]:
    """Return the training_dataset_* directory that contains the given model path, if any."""
    for parent in path.parents:
        if parent.name.startswith("training_dataset_"):
            return parent
    return None


def build_model(
    num_neurons: Tuple[Tuple[int, ...], ...],
    n_feat: int,
    target_name: str,
    out_act: str,
) -> MODNetModel:
    """Construct a MODNet model for regression."""
    return MODNetModel(
        targets=[[target_name]],
        weights={target_name: 1.0},
        num_neurons=num_neurons,
        n_feat=n_feat,
        num_classes={target_name: 0},
        out_act=out_act,
    )


def suggest_hyperparameters(trial: optuna.Trial) -> Dict[str, object]:
    """Sample hyperparameters for MODNet training."""
    params = {
        "batch_size": trial.suggest_categorical("batch_size", [32, 64, 128]),
        "learning_rate": trial.suggest_loguniform("learning_rate", 5e-5, 5e-3),
        "n_features": trial.suggest_int("n_features", 64, 256),
        "loss": trial.suggest_categorical("loss", ["mae"]),
        "out_act": trial.suggest_categorical("out_act", ["linear", "relu"]),
    }
    depth = trial.suggest_int("depth", 1, 4)
    width = trial.suggest_int("width", 64, 512, log=True)
    hidden = [[width] for _ in range(depth)]
    params["num_neurons"] = tuple(hidden) + ([],) * (4 - depth)
    return params


def evaluate_configuration(
    params: Dict[str, object],
    X: np.ndarray,
    y: np.ndarray,
    target_name: str,
    cv_splits: int,
    seed: int,
) -> Tuple[float, float]:
    """Run K-fold CV on the given configuration; return (mae_mean, mae_std)."""
    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=seed)
    maes: List[float] = []

    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X), start=1):
        X_train = X[train_idx][:, : params["n_features"]]
        X_val = X[val_idx][:, : params["n_features"]]
        y_train = y[train_idx]
        y_val = y[val_idx]

        md_train = make_moddata(X_train, y_train, target_name)
        md_val = make_moddata(X_val, y_val, target_name)

        model = build_model(
            num_neurons=params["num_neurons"],
            n_feat=params["n_features"],
            target_name=target_name,
            out_act=params["out_act"],
        )
        model.fit(
            md_train,
            batch_size=params["batch_size"],
            lr=params["learning_rate"],
            loss=params["loss"],
        )
        preds = flatten_predictions(model.predict(md_val, remap_out_of_bounds=False))
        maes.append(mean_absolute_error(y_val, preds))

    return float(np.mean(maes)), float(np.std(maes))


def objective_factory(
    X: np.ndarray,
    y: np.ndarray,
    target_name: str,
    cv_splits: int,
    seed: int,
    save_dir: Optional[Path] = None,
    context: Optional[Dict[str, object]] = None,
    X_test: Optional[np.ndarray] = None,
    y_test: Optional[np.ndarray] = None,
):
    """Create an Optuna objective function bound to the dataset."""

    def _objective(trial: optuna.Trial) -> float:
        params = suggest_hyperparameters(trial)
        mean_mae, std_mae = evaluate_configuration(params, X, y, target_name, cv_splits, seed)
        trial.set_user_attr("num_neurons", params["num_neurons"])
        trial.set_user_attr("mae_std", std_mae)
        if save_dir is not None:
            save_trial_model(
                params=params,
                X=X,
                y=y,
                target_name=target_name,
                trial_number=trial.number,
                out_dir=save_dir,
                cv_mean_mae=mean_mae,
                cv_std_mae=std_mae,
                context=context,
                X_test=X_test,
                y_test=y_test,
            )
        return mean_mae

    return _objective


def resolve_target_property(bundle: Dict, fallback: Optional[str]) -> str:
    """Determine target property name used during optimisation."""
    return str(bundle.get("target_property") or fallback or DEFAULT_TARGET_NAME)


def train_and_evaluate_best(
    params: Dict[str, object],
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    target_name: str,
) -> Dict[str, float]:
    """Train with best hyperparameters on the train split and evaluate on test."""
    md_train = make_moddata(X_train[:, : params["n_features"]], y_train, target_name)
    md_test = make_moddata(X_test[:, : params["n_features"]], y_test, target_name)

    model = build_model(
        num_neurons=params["num_neurons"],
        n_feat=params["n_features"],
        target_name=target_name,
        out_act=params["out_act"],
    )

    model.fit(
        md_train,
        batch_size=params["batch_size"],
        lr=params["learning_rate"],
        loss=params["loss"],
    )

    train_pred = flatten_predictions(model.predict(md_train, remap_out_of_bounds=False))
    test_pred = flatten_predictions(model.predict(md_test, remap_out_of_bounds=False))

    train_mae = mean_absolute_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    test_mae = mean_absolute_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)

    return {
        "train_mae": float(train_mae),
        "train_r2": float(train_r2),
        "test_mae": float(test_mae),
        "test_r2": float(test_r2),
    }


def save_trial_model(
    params: Dict[str, object],
    X: np.ndarray,
    y: np.ndarray,
    target_name: str,
    trial_number: int,
    out_dir: Path,
    cv_mean_mae: float,
    cv_std_mae: float,
    context: Optional[Dict[str, object]] = None,
    X_test: Optional[np.ndarray] = None,
    y_test: Optional[np.ndarray] = None,
) -> None:
    """Train and persist the MODNet model corresponding to an Optuna trial."""
    out_dir.mkdir(parents=True, exist_ok=True)
    trial_dir = out_dir / f"trial_{trial_number:04d}"
    trial_dir.mkdir(parents=True, exist_ok=True)

    md_full = make_moddata(X[:, : params["n_features"]], y, target_name)
    model = build_model(
        num_neurons=params["num_neurons"],
        n_feat=params["n_features"],
        target_name=target_name,
        out_act=params["out_act"],
    )
    model.fit(
        md_full,
        batch_size=params["batch_size"],
        lr=params["learning_rate"],
        loss=params["loss"],
    )

    train_pred = flatten_predictions(model.predict(md_full, remap_out_of_bounds=False))
    train_mae = mean_absolute_error(y, train_pred)
    train_r2 = r2_score(y, train_pred)

    test_mae: Optional[float] = None
    test_r2: Optional[float] = None
    if X_test is not None and y_test is not None:
        md_test = make_moddata(X_test[:, : params["n_features"]], y_test, target_name)
        test_pred = flatten_predictions(model.predict(md_test, remap_out_of_bounds=False))
        test_mae = float(mean_absolute_error(y_test, test_pred))
        test_r2 = float(r2_score(y_test, test_pred))

    model.save(str(trial_dir / "model.modnet"))

    serialisable_params: Dict[str, object] = {}
    for key, value in params.items():
        if key == "num_neurons":
            serialisable_params[key] = [list(block) for block in value]
        elif isinstance(value, (np.floating, np.integer)):
            serialisable_params[key] = float(value)
        else:
            serialisable_params[key] = value

    metadata: Dict[str, object] = {
        "trial_number": trial_number,
        "cv_mean_mae": float(cv_mean_mae),
        "cv_std_mae": float(cv_std_mae),
        "train_mae": float(train_mae),
        "train_r2": float(train_r2),
        "hyperparameters": serialisable_params,
    }
    if test_mae is not None and test_r2 is not None:
        metadata["test_mae"] = test_mae
        metadata["test_r2"] = test_r2

    if context:
        metadata.update(context)

    with open(trial_dir / "metadata.json", "w", encoding="utf-8") as meta_fp:
        json.dump(metadata, meta_fp, indent=2)

    if test_mae is not None and test_r2 is not None:
        print(
            f"[TRIAL {trial_number:04d}] Test MAE: {test_mae:.6f} | Test R2: {test_r2:.6f}"
        )


def save_trials_dataframe(
    study: optuna.Study,
    slug: str,
    layer: int,
    train_split: str,
    test_split: str,
):
    """Persist the Optuna trials dataframe for later analysis."""
    trials_df = study.trials_dataframe()
    out_path = HP_DIR / f"{slug}_{train_split}2{test_split}_l{layer}_optuna.csv"
    trials_df.to_csv(out_path, index=False)
    print(f"[INFO] Saved trial history: {out_path}")


def append_summary_row(row: Dict[str, object]) -> None:
    """Append optimisation summary to a CSV under HP_DIR."""
    summary_path = HP_DIR / "supercells_hp_summary.csv"
    write_header = not summary_path.exists()
    with open(summary_path, "a", encoding="utf-8", newline="") as csv_fp:
        writer = csv.DictWriter(csv_fp, fieldnames=row.keys())
        if write_header:
            writer.writeheader()
        writer.writerow(row)
    print(f"[INFO] Updated summary: {summary_path}")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Optuna-based hyperparameter optimisation for MODNet using supercell features."
    )
    parser.add_argument(
        "--feature-slugs",
        default=None,
        help="Comma-separated dataset slugs. Overrides --tasks when provided.",
    )
    parser.add_argument(
        "--tasks",
        default=None,
        help="Comma-separated task slugs (fallback if --feature-slugs not provided).",
    )
    parser.add_argument(
        "--train-split",
        default="train",
        help="Split name used for optimisation (default: train).",
    )
    parser.add_argument(
        "--test-split",
        default="test",
        help="Split used for reporting evaluation metrics (default: test).",
    )
    parser.add_argument(
        "--key",
        default=DEFAULT_KEY,
        help="Feature key prefix (default: XPS).",
    )
    parser.add_argument(
        "--layers",
        default=None,
        help="Comma-separated list of layers to evaluate (e.g. '1,2,3').",
    )
    parser.add_argument(
        "--layer",
        type=int,
        default=None,
        help="Specify a single layer to evaluate (overrides --layers and min/max bounds).",
    )
    parser.add_argument(
        "--min-layer",
        type=int,
        default=min(DEFAULT_LAYERS),
        help="Lower bound for layer scan when --layers not provided.",
    )
    parser.add_argument(
        "--max-layer",
        type=int,
        default=max(DEFAULT_LAYERS),
        help="Upper bound for layer scan when --layers not provided.",
    )
    parser.add_argument(
        "--n-trials",
        type=int,
        default=DEFAULT_OPTUNA_TRIALS,
        help="Number of Optuna trials per layer.",
    )
    parser.add_argument(
        "--cv-folds",
        type=int,
        default=5,
        help="Number of CV folds used inside optimisation.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed.",
    )
    parser.add_argument(
        "--cuda-visible-devices",
        default=None,
        help="CUDA_VISIBLE_DEVICES value to set before optimisation.",
    )
    parser.add_argument(
        "--n-jobs",
        type=int,
        default=1,
        help="Parallelism for Optuna (number of worker threads).",
    )
    parser.add_argument(
        "--sampler",
        default="tpe",
        choices=["tpe", "random"],
        help="Optuna sampler to use (default: tpe).",
    )
    parser.add_argument(
        "--early-stop-trials",
        type=int,
        default=15,
        help="Early-stopping patience (trials without improvement).",
    )
    parser.add_argument(
        "--model-path",
        type=Path,
        default=None,
        help="Optional MODNet model file (e.g. *_XPS_orb2_l11.modnet) used to "
             "infer the dataset slug/splits/feature key and default layer. Overrides "
             "--feature-slugs and provides a default layer when explicit layers are not supplied.",
    )
    parser.add_argument(
        "--trained-model-path",
        type=Path,
        default=None,
        help="Path to a model saved by train_modnet_from_supercells.py; used to infer "
             "dataset context and store Optuna trial models alongside the training run.",
    )
    args = parser.parse_args()

    if args.model_path is not None and args.trained_model_path is not None:
        raise ValueError("Please specify only one of --model-path or --trained-model-path.")

    model_layer_override: Optional[int] = None
    model_slug_override: Optional[str] = None
    model_key_override: Optional[str] = None
    model_mlip_override: Optional[str] = None
    model_train_split_override: Optional[str] = None
    model_test_split_override: Optional[str] = None
    trial_models_base = TRIAL_MODELS_DIR

    model_reference_path: Optional[Path] = None
    if args.trained_model_path is not None:
        model_reference_path = args.trained_model_path.expanduser().resolve()
    elif args.model_path is not None:
        model_reference_path = args.model_path.expanduser().resolve()

    if model_reference_path is not None:
        if not model_reference_path.exists():
            raise FileNotFoundError(f"Specified model path does not exist: {model_reference_path}")

        context = extract_context_from_model_path(model_reference_path)
        model_layer_override = int(context["layer"])
        model_slug_override = str(context["slug"])
        model_key_override = str(context["key"])
        model_mlip_override = str(context["mlip"])
        model_train_split_override = str(context["train_split"])
        model_test_split_override = str(context["test_split"])

        run_root = locate_training_run_root(model_reference_path)
        if run_root is not None:
            trial_models_base = run_root / "trial_models"

        if model_mlip_override.lower() != MLIP.lower():
            print(
                f"[WARN] Model MLIP '{model_mlip_override}' differs from environment MLIP '{MLIP}'."
            )

        print(
            "[INFO] Inferred from model → "
            f"slug='{model_slug_override}', train_split='{model_train_split_override}', "
            f"test_split='{model_test_split_override}', key='{model_key_override}', "
            f"layer={model_layer_override}"
        )

        if model_train_split_override and args.train_split == DEFAULT_TRAIN_SPLIT:
            args.train_split = model_train_split_override
        elif (
            model_train_split_override
            and args.train_split != model_train_split_override
        ):
            print(
                "[WARN] Using train split "
                f"'{args.train_split}' (model suggests '{model_train_split_override}')."
            )

        if model_test_split_override and args.test_split == DEFAULT_TEST_SPLIT:
            args.test_split = model_test_split_override
        elif (
            model_test_split_override
            and args.test_split != model_test_split_override
        ):
            print(
                "[WARN] Using test split "
                f"'{args.test_split}' (model suggests '{model_test_split_override}')."
            )

        inferred_key = model_key_override.upper()
        current_key = args.key.upper()
        if current_key == DEFAULT_KEY:
            args.key = inferred_key
        elif current_key != inferred_key:
            print(
                "[WARN] Using feature key "
                f"'{args.key.upper()}' (model suggests '{inferred_key}')."
            )

    feature_spec = args.feature_slugs or os.environ.get("BENCH_FEATURE_SLUGS")
    explicit_slugs = parse_slug_list(feature_spec)
    if model_slug_override is not None:
        if explicit_slugs and model_slug_override not in explicit_slugs:
            raise ValueError(
                f"Model slug '{model_slug_override}' not present in --feature-slugs "
                f"({explicit_slugs})."
            )
        target_slugs = [model_slug_override]
    else:
        if explicit_slugs:
            target_slugs = explicit_slugs
        else:
            task_spec = args.tasks if args.tasks is not None else TASKS_ENV
            target_slugs = parse_task_list(task_spec)

    configure_devices(args.cuda_visible_devices)
    set_seed(args.seed)

    key = args.key.upper()
    if model_key_override is not None:
        key = model_key_override.upper()

    if args.layer is not None:
        requested_layers = [args.layer]
    elif args.layers:
        requested_layers = [
            int(tok.strip()) for tok in args.layers.split(",") if tok.strip().isdigit()
        ]
    else:
        requested_layers = []
    if model_layer_override is not None and not requested_layers:
        requested_layers = [model_layer_override]

    requested_layers = sorted(set(requested_layers))

    sampler = (
        optuna.samplers.TPESampler(seed=args.seed)
        if args.sampler == "tpe"
        else optuna.samplers.RandomSampler(seed=args.seed)
    )

    for slug in target_slugs:
        print(f"[INFO] Optimising dataset: {slug}")
        train_bundle, train_path = load_feature_bundle(slug, key, args.train_split)
        test_bundle, test_path = load_feature_bundle(slug, key, args.test_split)

        target_property = resolve_target_property(train_bundle, DEFAULT_TARGET_NAME)
        print(f"[INFO] Target property: {target_property}")
        print(f"[INFO] Train bundle: {train_path}")
        print(f"[INFO] Test bundle:  {test_path}")

        train_layers = detect_available_layers(train_bundle, key)
        test_layers = detect_available_layers(test_bundle, key)
        available_layers = sorted(set(train_layers) & set(test_layers))
        if not available_layers:
            raise RuntimeError(f"No common layers between train/test bundles for {slug}.")

        if requested_layers:
            candidate_layers = [layer for layer in requested_layers if layer in available_layers]
            if not candidate_layers:
                raise ValueError(
                    f"Requested layers {requested_layers} not available. Found layers: {available_layers}"
                )
        else:
            lower = max(args.min_layer, min(available_layers, default=args.min_layer))
            upper = min(args.max_layer, max(available_layers, default=args.max_layer))
            candidate_layers = [layer for layer in available_layers if lower <= layer <= upper]

        print(f"[INFO] Candidate layers: {candidate_layers}")

        y_train = np.asarray(train_bundle["targets"], dtype=np.float32)
        y_test = np.asarray(test_bundle["targets"], dtype=np.float32)

        best_rows: List[Dict[str, object]] = []

        for layer in candidate_layers:
            print(f"[LAYER {layer}] Starting optimisation …")

            X_train = get_layer_features(train_bundle, key, layer)
            X_test = get_layer_features(test_bundle, key, layer)

            trial_models_dir = (
                trial_models_base
                / slug
                / f"{args.train_split}2{args.test_split}"
                / f"l{layer:02d}"
            )
            trial_context = {
                "dataset": slug,
                "train_split": args.train_split,
                "test_split": args.test_split,
                "layer": layer,
                "feature_key": key,
                "mlip": MLIP,
                "model_type": MODEL,
                "target_property": target_property,
            }

            objective = objective_factory(
                X=X_train,
                y=y_train,
                target_name=target_property,
                cv_splits=args.cv_folds,
                seed=args.seed,
                save_dir=trial_models_dir,
                context=trial_context,
                X_test=X_test,
                y_test=y_test,
            )

            study = optuna.create_study(direction="minimize", sampler=sampler)

            es_state = {"best": float("inf"), "stale": 0}

            def early_stop_callback(study_obj: optuna.Study, trial_obj: optuna.trial.FrozenTrial) -> None:
                nonlocal es_state
                if trial_obj.value < es_state["best"]:
                    es_state["best"] = trial_obj.value
                    es_state["stale"] = 0
                else:
                    es_state["stale"] += 1
                    if es_state["stale"] >= args.early_stop_trials:
                        print(
                            f"[INFO] Early stopping after {es_state['stale']} stale trials "
                            f"(layer {layer})."
                        )
                        study_obj.stop()

            study.optimize(
                objective,
                n_trials=args.n_trials,
                n_jobs=args.n_jobs,
                callbacks=[early_stop_callback],
                show_progress_bar=args.n_jobs == 1,
            )

            save_trials_dataframe(study, slug, layer, args.train_split, args.test_split)

            best_params = study.best_trial.params
            best_params["num_neurons"] = study.best_trial.user_attrs.get(
                "num_neurons", suggest_hyperparameters(study.best_trial)["num_neurons"]
            )

            # user_attrs may not include num_neurons; ensure we recover them from trial state
            if "num_neurons" not in study.best_trial.user_attrs:
                hidden_blocks = tuple(
                    ([[study.best_params["width"]]] if idx < study.best_params["depth"] else [])
                    for idx in range(4)
                )
                best_params["num_neurons"] = hidden_blocks

            if "loss" not in best_params:
                best_params["loss"] = "mae"
            if "out_act" not in best_params:
                best_params["out_act"] = "linear"

            # Evaluate best params on train/test split
            metrics = train_and_evaluate_best(
                params=best_params,
                X_train=X_train,
                y_train=y_train,
                X_test=X_test,
                y_test=y_test,
                target_name=target_property,
            )

            best_row = {
                "dataset": slug,
                "train_split": args.train_split,
                "test_split": args.test_split,
                "layer": layer,
                "train_mae": metrics["train_mae"],
                "train_r2": metrics["train_r2"],
                "test_mae": metrics["test_mae"],
                "test_r2": metrics["test_r2"],
                "batch_size": best_params["batch_size"],
                "learning_rate": best_params["learning_rate"],
                "n_features": best_params["n_features"],
                "depth": sum(len(block) > 0 for block in best_params["num_neurons"]),
                "width": best_params["num_neurons"][0][0] if best_params["num_neurons"][0] else 0,
                "loss": best_params["loss"],
                "out_act": best_params["out_act"],
                "optuna_best_value": study.best_value,
                "optuna_best_mae_std": study.best_trial.user_attrs.get("mae_std"),
                "n_trials": len(study.trials),
            }

            best_rows.append(best_row)
            append_summary_row(best_row)

        if best_rows:
            best_rows.sort(key=lambda row: row["test_mae"])
            best_overall = best_rows[0]
            print(
                f"[BEST] Dataset {slug} | Layer {best_overall['layer']} | "
                f"Test MAE {best_overall['test_mae']:.6f} | Test R2 {best_overall['test_r2']:.6f}"
            )


if __name__ == "__main__":
    main()
